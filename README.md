# Case T√©cnico de Engenharia de Dados - iFood (NYC Taxi)

![PySpark](https://img.shields.io/badge/PySpark-FB542B?style=for-the-badge&logo=apachespark&logoColor=white)
![dbt](https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=for-the-badge&logo=terraform&logoColor=white)
![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-2088FF?style=for-the-badge&logo=githubactions&logoColor=white)

Este reposit√≥rio cont√©m a solu√ß√£o completa para o case t√©cnico de engenharia de dados proposto pelo iFood, focado na ingest√£o, modelagem e an√°lise dos dados de corridas de t√°xis de Nova York.

O projeto foi constru√≠do com √™nfase em pr√°ticas modernas de engenharia de dados, destacando:
- **Qualidade e Governan√ßa:** Atrav√©s de Contratos de Dados e testes automatizados.
- **Automa√ß√£o de Ponta a Ponta:** Utilizando Infraestrutura como C√≥digo (IaC) e CI/CD.
- **Arquitetura Escal√°vel:** Seguindo o padr√£o ELT e a metodologia Medallion.

## üìú Documenta√ß√£o Viva do Projeto (dbt Docs)

Uma documenta√ß√£o completa, interativa e sempre atualizada do nosso pipeline de dados est√° dispon√≠vel online via GitHub Pages. Ela inclui descri√ß√µes de modelos, testes de qualidade, e um gr√°fico de linhagem de dados completo.

**[‚û°Ô∏è Acesse a Documenta√ß√£o Viva aqui](https://packland.github.io/ifood-data-eng-case/)**

## üèóÔ∏è Arquitetura da Solu√ß√£o

Adotamos o padr√£o **ELT (Extract, Load, Transform)** e a **Arquitetura Medallion** para garantir uma clara separa√ß√£o de responsabilidades e a progress√£o da qualidade dos dados atrav√©s do pipeline.

```mermaid
graph LR
    subgraph "Fonte de Dados"
        A[Arquivos Parquet<br>NYC TLC]
    end

    subgraph "Camada Bronze (Ingest√£o)"
        B(PySpark em<br>Databricks Serverless)
    end

    subgraph "Camada Silver (Limpeza & Conformidade)"
        C(dbt Model<br>silver)
    end

    subgraph "Camada Gold (Business-Ready)"
        D(dbt Model<br>gold_obt_taxi_trips)
        E(dbt Model<br>gold_monthly_metrics)
        F(dbt Model<br>gold_hourly_metrics)
    end
    
    subgraph "Consumo & Governan√ßa"
        G[An√°lises<br>em SQL]
        H[Documenta√ß√£o<br>dbt Docs]
    end

    A --> B
    B -- Load --> C
    C -- Transform --> D
    C -- Transform --> E
    C -- Transform --> F
    E --> G
    F --> G
```

- **Bronze:** A ingest√£o dos dados brutos √© feita por um notebook PySpark, orquestrado por um Job no Databricks. Os dados s√£o armazenados como Delta Tables sem transforma√ß√µes complexas.
- **Silver:** O dbt assume a responsabilidade, aplicando limpezas, convers√£o de tipos e testes de qualidade para criar uma tabela confi√°vel e audit√°vel.
- **Gold:** O dbt cria *data marts* agregados e denormalizados, prontos para responder diretamente √†s perguntas de neg√≥cio com m√°xima performance.

### Otimiza√ß√µes de Performance (Particionamento)

Para garantir a performance e a escalabilidade das consultas, especialmente na camada Silver que cont√©m um grande volume de dados, uma otimiza√ß√£o chave foi implementada:

-   **Particionamento F√≠sico:** As tabelas `silver` e `gold_obt_taxi_trips` foram fisicamente particionadas pela coluna `pickup_date`. Isso significa que os dados s√£o armazenados em subdiret√≥rios organizados por data no Delta Lake. Quando uma consulta filtra por um per√≠odo espec√≠fico (ex: um m√™s ou uma semana), o Databricks ignora todos os outros diret√≥rios, lendo uma quantidade drasticamente menor de dados e acelerando as transforma√ß√µes e an√°lises subsequentes. Esta otimiza√ß√£o foi aplicada diretamente no modelo dbt atrav√©s da configura√ß√£o `partition_by`.

## ‚ú® Foco em Qualidade e Governan√ßa

A confiabilidade dos dados foi o pilar central deste projeto, garantida atrav√©s de:

- **Contratos de Dados (`schema.yml`):** Cada tabela nas camadas Silver e Gold possui um "contrato" que define a estrutura esperada (nome e tipo de cada coluna) e √© for√ßado em tempo de execu√ß√£o com `contract: enforced: true`. Isso previne que dados malformados sejam propagados.
- **Testes Automatizados:** Testes automatizados (implementados com dbt) validam a integridade dos dados, incluindo:
  - `unique` e `not_null` para chaves prim√°rias.
  - `accepted_values` para campos categ√≥ricos.
  - Testes de express√£o para garantir a ader√™ncia a regras de neg√≥cio (ex: `total_amount >= 0`).
- **Documenta√ß√£o como Produto:** A documenta√ß√£o n√£o √© um artefato est√°tico, mas um produto vivo, gerado e publicado automaticamente a cada atualiza√ß√£o do pipeline, garantindo que a governan√ßa e o conhecimento sobre os dados estejam sempre acess√≠veis e atualizados.

## ‚öôÔ∏è Automa√ß√£o de Ponta a Ponta (CI/CD)

Todo o ciclo de vida do pipeline, desde a infraestrutura at√© a publica√ß√£o da documenta√ß√£o, √© 100% automatizado, eliminando a necessidade de interven√ß√£o manual e garantindo consist√™ncia.

- **Infraestrutura como C√≥digo (IaC) com Terraform:** O notebook de ingest√£o e o Job Serverless no Databricks s√£o definidos e gerenciados via Terraform. O estado da infraestrutura √© mantido no Terraform Cloud, seguindo as melhores pr√°ticas de IaC para ambientes de CI/CD.
- **Orquestra√ß√£o com GitHub Actions:** Um workflow completo (`.github/workflows/end_to_end_pipeline.yml`) orquestra todo o processo a cada `push` na branch `main`:
    1.  **Deploy da Infra:** `terraform apply` garante que o job no Databricks esteja configurado corretamente.
    2.  **Execu√ß√£o do Bronze:** Inicia o job de ingest√£o PySpark e aguarda sua conclus√£o com sucesso.
    3.  **Execu√ß√£o de Silver/Gold:** Roda `dbt build` para construir e testar todas as tabelas transformadas.
    4.  **Deploy da Documenta√ß√£o:** Roda `dbt docs generate` e publica o site atualizado no GitHub Pages.

## üöÄ Como Executar o Projeto

### Pr√©-requisitos

1.  **Conta Databricks:** Acesso a um workspace Databricks.
2.  **Conta Terraform Cloud:** Uma conta no plano gratuito √© suficiente para gerenciar o estado.
3.  **Secrets do GitHub:** Configure os seguintes secrets no seu reposit√≥rio:
    - `DATABRICKS_HOST`: A URL do seu workspace.
    - `DATABRICKS_TOKEN`: Um Personal Access Token do Databricks.
    - `DATABRICKS_HTTP_PATH`: O HTTP Path do seu SQL Warehouse Serverless.
    - `TF_API_TOKEN`: Um token de API gerado no Terraform Cloud.

### Execu√ß√£o Automatizada (Recomendado)

A forma mais simples de executar o projeto √© atrav√©s do workflow de CI/CD:

1.  Fa√ßa um fork deste reposit√≥rio.
2.  Configure os secrets do GitHub conforme listado acima.
3.  Ajuste a organiza√ß√£o e o workspace do Terraform Cloud no arquivo `main.tf`.
4.  Fa√ßa um `commit` e `push` para a branch `main`. O GitHub Actions ir√° iniciar o pipeline completo automaticamente.

## üìÇ Estrutura do Reposit√≥rio

```
‚îú‚îÄ‚îÄ .github/workflows/        # Defini√ß√£o do pipeline de CI/CD.
‚îú‚îÄ‚îÄ analysis/                 # Scripts SQL com as respostas finais do case.
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ bronze_ingestion/     # C√≥digo fonte da ingest√£o (Notebook).
‚îÇ   ‚îî‚îÄ‚îÄ dbt_project/          # Projeto dbt com todos os modelos e testes.
‚îú‚îÄ‚îÄ main.tf                   # Defini√ß√£o da infraestrutura (Job Databricks) via Terraform.
‚îî‚îÄ‚îÄ README.md
```

## üìà Respostas para as Perguntas do Case

As querys finais que respondem √†s perguntas do desafio est√£o na pasta `analysis/`. Elas consultam diretamente as tabelas da camada Gold, demonstrando a simplicidade do consumo dos dados ap√≥s o trabalho de engenharia.

#### 1. Qual a m√©dia de valor total (`total_amount`) recebido em um m√™s?
*Arquivo: [`analysis/q1_media_valor_total_por_mes.sql`](./analysis/q1_media_valor_total_por_mes.sql)*

| mes_ano    | media_valor_total    |
| :--------- | :------------------- |
| 2023-01-01 | 27.459212356156897   |
| 2023-02-01 | 27.36534006050982    |
| 2023-03-01 | 28.28459309096031    |
| 2023-04-01 | 28.780344804726806   |
| 2023-05-01 | 29.449597559178017   |


#### 2. Qual a m√©dia de passageiros (`passenger_count`) por cada hora do dia no m√™s de maio?
*Arquivo: [`analysis/q2_media_passageiros_por_hora_dia.sql`](./analysis/q2_media_passageiros_por_hora_dia.sql)*

| hora_do_dia | media_passageiros    |
| :---------- | :------------------- |
| 0           | 1.427422724654703    |
| 1           | 1.438027164744961    |
| 2           | 1.4553930974838518   |
| 3           | 1.4524155693100154   |
| 4           | 1.404934503370215    |
| 5           | 1.2844495765973827   |
| 6           | 1.2612810917895663   |
| 7           | 1.2820630247519353   |
| 8           | 1.295685461360555    |
| 9           | 1.3121342121711461   |
| 10          | 1.347728916486939    |
| 11          | 1.3623017676465663   |
| 12          | 1.3761908987056775   |
| 13          | 1.3851633398748793   |
| 14          | 1.3902040165127734   |
| 15          | 1.4019182896470932   |
| 16          | 1.3991033796440837   |
| 17          | 1.3900312110698834   |
| 18          | 1.3836559916964672   |
| 19          | 1.3922698215104687   |
| 20          | 1.4012711016565393   |
| 21          | 1.420006593995343    |
| 22          | 1.4278717844427482   |
| 23          | 1.4225079467123827   |