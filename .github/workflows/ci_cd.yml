# .github/workflows/ci_cd.yml

name: CI/CD Pipeline for dbt Transformations

on:
  push:
    branches:
      - main

permissions:
  id-token: write
  contents: read

jobs:
  run-dbt-on-databricks:
    runs-on: ubuntu-latest
    environment: prod
    
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_CLIENT_ID: ${{ secrets.DATABRICKS_CLIENT_ID }}
      DATABRICKS_HTTP_PATH: ${{ secrets.DATABRICKS_HTTP_PATH }}
      DBT_DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      # Garante que o dbt não peça feedback
      DBT_SEND_ANONYMOUS_USAGE_STATS: "false"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Notebook on a New Job Cluster
        uses: databricks/run-notebook@v0
        with:
          # A action usará as variáveis de ambiente DATABRICKS_HOST e DATABRICKS_CLIENT_ID para autenticação OIDC
          
          # O caminho para nosso notebook de teste
          workspace-notebook-path: /Workspace/Users/packland@gmail.com/teste_conectividade_externa

          # A MÁGICA: Em vez de usar um cluster existente, definimos um novo cluster temporário.
          # Este cluster será criado, executará o notebook e será terminado automaticamente.
          new-cluster-json: >
            {
              "num_workers": 1,
              "spark_version": "13.3.x-scala2.12",
              "node_type_id": "Standard_DS3_v2"
            }