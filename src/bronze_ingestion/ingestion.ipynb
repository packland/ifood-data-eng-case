{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb2cc6",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG workspace;\n",
    "USE SCHEMA case_ifood;\n",
    "\n",
    "CREATE VOLUME IF NOT EXISTS raw_taxi_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49950789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# --- 1. Configura√ß√£o ---\n",
    "TAXI_TYPE = \"yellow\"\n",
    "YEAR = 2023\n",
    "MONTHS_LIST = list(range(1, 6))\n",
    "BASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/{type}_tripdata_{year}-{month:02d}.parquet\"\n",
    "\n",
    "# --- 2. Defini√ß√£o do Caminho do Volume ---\n",
    "# Este √© o novo caminho padr√£o para acessar o Volume que criamos.\n",
    "# √â um caminho de sistema de arquivos simples e direto.\n",
    "CATALOG_NAME = \"workspace\"\n",
    "SCHEMA_NAME = \"case_ifood\"\n",
    "VOLUME_NAME = \"raw_taxi_data\"\n",
    "\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}/{YEAR}/\"\n",
    "\n",
    "print(f\"Diret√≥rio de destino no Volume: {VOLUME_PATH}\")\n",
    "\n",
    "# Cria o subdiret√≥rio do ano dentro do volume se ele n√£o existir\n",
    "os.makedirs(VOLUME_PATH, exist_ok=True)\n",
    "print(\"‚úÖ Diret√≥rio de destino garantido.\")\n",
    "\n",
    "# --- 3. Loop de Download ---\n",
    "for month in MONTHS_LIST:\n",
    "    file_name = f\"{TAXI_TYPE}_tripdata_{YEAR}-{month:02d}.parquet\"\n",
    "    file_path = os.path.join(VOLUME_PATH, file_name)\n",
    "    url = BASE_URL.format(type=TAXI_TYPE, year=YEAR, month=month)\n",
    "    \n",
    "    # Verifica se o arquivo j√° existe para n√£o baixar novamente\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"üü° Arquivo '{file_name}' j√° existe. Pulando.\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        print(f\"üîÑ Baixando '{file_name}'...\")\n",
    "        \n",
    "        # Faz o download de forma eficiente usando bibliotecas padr√£o\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192): \n",
    "                    f.write(chunk)\n",
    "        \n",
    "        print(f\"‚úÖ Arquivo '{file_name}' salvo com sucesso em {file_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRO ao baixar o m√™s {month}: {e}\")\n",
    "\n",
    "print(\"\\n‚ú® Etapa de download para o Volume do Unity Catalog conclu√≠da.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d015da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# --- 1. Configura√ß√£o ---\n",
    "CATALOG_NAME = \"workspace\"\n",
    "SCHEMA_NAME = \"case_ifood\"\n",
    "VOLUME_NAME = \"raw_taxi_data\"\n",
    "YEAR = \"2023\"\n",
    "\n",
    "SOURCE_DIR = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}/{YEAR}/\"\n",
    "TABLE_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.bronze\"\n",
    "\n",
    "# --- 2. Defini√ß√£o do Esquema Fixo e Normalizado (Contrato de Dados) ---\n",
    "official_columns_lower = [\n",
    "    \"vendorid\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "    \"passenger_count\", \"trip_distance\", \"ratecodeid\", \"store_and_fwd_flag\",\n",
    "    \"pulocationid\", \"dolocationid\", \"payment_type\", \"fare_amount\", \"extra\",\n",
    "    \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\",\n",
    "    \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"cbd_congestion_fee\"\n",
    "]\n",
    "print(\"‚úÖ Contrato de dados com a lista oficial de colunas (em min√∫sculas) definido.\")\n",
    "\n",
    "# --- 3. Processamento Robusto: Loop, Conformidade e Uni√£o ---\n",
    "try:\n",
    "    files_to_process = [f.path for f in dbutils.fs.ls(SOURCE_DIR) if f.path.endswith('.parquet')]\n",
    "    list_of_conformed_dfs = []\n",
    "\n",
    "    print(f\"Encontrados {len(files_to_process)} arquivos para processar.\")\n",
    "\n",
    "    for file_path in files_to_process:\n",
    "        print(f\"  - Processando arquivo: {file_path}\")\n",
    "        temp_df = spark.read.parquet(file_path)\n",
    "        temp_df_lower = temp_df.toDF(*[c.lower() for c in temp_df.columns])\n",
    "        \n",
    "        select_exprs = []\n",
    "        temp_df_columns_set = set(temp_df_lower.columns)\n",
    "        \n",
    "        for column_name in official_columns_lower:\n",
    "            if column_name in temp_df_columns_set:\n",
    "                select_exprs.append(col(column_name).cast(StringType()).alias(column_name))\n",
    "            else:\n",
    "                select_exprs.append(lit(None).cast(StringType()).alias(column_name))\n",
    "        \n",
    "        conformed_df = temp_df_lower.select(select_exprs)\n",
    "        list_of_conformed_dfs.append(conformed_df)\n",
    "\n",
    "    print(\"\\n‚úÖ Todos os arquivos foram lidos e conformados ao schema oficial.\")\n",
    "\n",
    "    # c. Unir todos os DataFrames (vers√£o compat√≠vel com Spark Connect)\n",
    "    if not list_of_conformed_dfs:\n",
    "        raise ValueError(\"Nenhum DataFrame para unir. Verifique se os arquivos existem.\")\n",
    "    \n",
    "    # Inicializa o DataFrame final com o primeiro da lista\n",
    "    bronze_df = list_of_conformed_dfs[0]\n",
    "\n",
    "    # Faz a uni√£o com os DataFrames restantes em um loop 'for' padr√£o\n",
    "    for i in range(1, len(list_of_conformed_dfs)):\n",
    "        bronze_df = bronze_df.unionByName(list_of_conformed_dfs[i])\n",
    "\n",
    "    print(\"\\n‚úÖ Todos os DataFrames foram unidos com sucesso.\")\n",
    "    \n",
    "    # --- 4. Escrita na Camada Bronze ---\n",
    "    (\n",
    "        bronze_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(TABLE_NAME)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚ú® Sucesso! Dados processados e salvos na tabela '{TABLE_NAME}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO durante o processamento com Spark: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
